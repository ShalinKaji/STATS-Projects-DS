1+1
install.packages("KernSmooth")
library(kernSmooth)
library(KernSmooth)
install.packages("devtools")
library(devtools)
# STATS-Mini-Project-5
# Setting working directory to Proj-5 folder.
setwd("C:/Users/Shalin Kaji/Desktop/UT-Dallas-Spr23/STATS-DS-Min.Chen/Mini-Proj-5")
getwd()
library(BSDA)
# Exploring our .csv file
data <- read.csv("bodytemp-heartrate.csv")
typeof(data$gender)
mdata <- data[data$gender==1,]
fdata <- data[data$gender==2,]
# Question-1(a)
# Hypothesis Testing for heart_rate metric for M and F data.
# Since number of obv is 65 we can assume that it follows Normal Distribution from the Law of Large Numbers.
print('For finding the relation between Variance of two datasets.')
var.test(mdata$heart_rate, fdata$heart_rate, alternative="two.sided")
print('Varinaces are unequal as shown by Fischer\'s test on M-F data.')
z.test(x=mdata$heart_rate,y=fdata$heart_rate,mu=0,sigma.x=sd(mdata$heart_rate),sigma.y=sd(fdata$heart_rate),alternative="two.sided")
# Question-1(b)
# Hypothesis Testing for body_temp metric for M and F data.
# Since number of obv is 65 we can assume that it follows Normal Distribution from the Law of Large Numbers.
print('For finding the relation between Variance of two datasets.')
var.test(mdata$body_temperature, fdata$body_temperature, alternative="two.sided")
print('Varinaces are unequal as shown by Fischer\'s test on M-F data.')
z.test(x=mdata$body_temperature,y=fdata$body_temperature,mu=0,sigma.x=sd(mdata$body_temperature),sigma.y=sd(fdata$body_temperature),alternative="two.sided",conf.level = 0.99)
# Question-1(c)
rho <- cor(mdata$heart_rate,mdata$body_temperature) # finding Pearson's correlation coeff.
rho
# found the point estimate of rho.
ggpubr::ggscatter(mdata, x = "heart_rate", y = "body_temperature",
add = "reg.line", conf.int = TRUE,
cor.coef = TRUE, cor.method = "pearson",
xlab = "Heart Rate", ylab = "Body Temperature")
rho <- cor(fdata$heart_rate,fdata$body_temperature) # finding Pearson's correlation coeff.
rho
# found the point estimate of rho.
ggpubr::ggscatter(fdata, x = "heart_rate", y = "body_temperature",
add = "reg.line", conf.int = TRUE,
cor.coef = TRUE, cor.method = "pearson",
xlab = "Heart Rate", ylab = "Body Temperature")
print("\n For finding the relation between the means of the male and female sample populations.")
mean.test(mdata$heart_rate, fdata$heart_rate, alternative="two.sided")
help("t.test")
help(z.test)
# Hypothesis Testing for heart_rate metric for M and F data.
# Since number of obv is 65 we can assume that it follows Normal Distribution from the Law of Large Numbers.
print('Mean BodyTemps are unequal as shown by Fischer\'s test on M-F data.')
z.test(x=mdata$heart_rate,y=fdata$heart_rate,mu=0,sigma.x=sd(mdata$heart_rate),sigma.y=sd(fdata$heart_rate),alternative="two.sided")
summary(mdata)
summary(fdata)
# Hypothesis Testing for heart_rate metric for M and F data.
# Since number of obv is 65 we can assume that it follows Normal Distribution from the Law of Large Numbers.
print('Mean BodyTemps are unequal as shown by Fischer\'s test on M-F data.')
z.test(x=mdata$heart_rate,y=fdata$heart_rate,mu=0,sigma.x=sd(mdata$heart_rate),sigma.y=sd(fdata$heart_rate),alternative="two.sided")
summary(mdata$body_temperature)
summary(fdata$body_temperature)
# Exploring our .csv file
data <- read.csv("bodytemp-heartrate.csv")
typeof(data$gender)
mdata <- data[data$gender==1,]
fdata <- data[data$gender==2,]
# Question-1(a)
# Hypothesis Testing for body_temp metric for M and F data.
# Since number of obv is 65 we can assume that it follows Normal Distribution from the Law of Large Numbers.
print('Mean BodyTemps are unequal as shown by z-test on M-F data.')
z.test(x=mdata$body_temperature,y=fdata$body_temperature,mu=0,sigma.x=sd(mdata$body_temperature),sigma.y=sd(fdata$body_temperature),alternative="two.sided")
summary(mdata$body_temperature)
summary(fdata$body_temperature)
# Hypothesis Testing for heart_rate metric for M and F data.
# Since number of obv is 65 we can assume that it follows Normal Distribution from the Law of Large Numbers.
print('Mean Heart-Rates are unequal as shown by z-test on M-F data.')
z.test(x=mdata$heart_rate,y=fdata$heart_rate,mu=0,sigma.x=sd(mdata$heart_rate),sigma.y=sd(fdata$heart_rate),alternative="two.sided",conf.level = 0.99)
summary(mdata$heart_rate)
summary(fdata$heart_rate)
# Question-1(c)
rho <- cor(mdata$heart_rate,mdata$body_temperature) # finding Pearson's correlation coeff.
rho
# found the point estimate of rho.
ggpubr::ggscatter(mdata, x = "heart_rate", y = "body_temperature",
add = "reg.line", conf.int = TRUE,
cor.coef = TRUE, cor.method = "pearson",
xlab = "Heart Rate", ylab = "Body Temperature")
rho <- cor(fdata$heart_rate,fdata$body_temperature) # finding Pearson's correlation coeff.
rho
# found the point estimate of rho.
ggpubr::ggscatter(fdata, x = "heart_rate", y = "body_temperature",
add = "reg.line", conf.int = TRUE,
cor.coef = TRUE, cor.method = "pearson",
xlab = "Heart Rate", ylab = "Body Temperature")
# Generate data
data <- generate_data(30)
set.seed(123)
# Function to generate data from normal distribution
generate_data <- function(n) {
rexp(n,.01)
}
# Function to compute large sample z confidence interval
z_conf_interval <- function(data, alpha) {
n <- length(data)
se <- sd(data)/sqrt(n)
z <- qnorm(1-alpha/2)
lower <- mean(data) - z*se
upper <- mean(data) + z*se
return(c(lower, upper))
}
# Function to compute parametric bootstrap confidence interval using boot function
boot_conf_interval <- function(data, alpha) {
boot_fn <- function(data, indices) {
mean(data[indices])
}
boot_samples <- boot(data, boot_fn, R=1000)
lower <- quantile(boot_samples$t, alpha/2)
upper <- quantile(boot_samples$t, 1-alpha/2)
return(c(lower, upper))
}
# Simulation settings
n <- 10
mean <- 100
sd <- 2
alpha <- 0.05
simulations <- 1000
# Initialize matrices to store coverage probabilities
z_coverage <- numeric(simulations)
boot_coverage <- numeric(simulations)
# Run Monte-Carlo simulation
for(i in 1:simulations) {
# Generate data
data <- generate_data(30)
# Compute confidence intervals
z_ci <- z_conf_interval(data, alpha)
boot_ci <- boot_conf_interval(data, alpha)
# Check if true mean is within confidence intervals
z_coverage[i] <- (mean >= z_ci[1]) & (mean <= z_ci[2])
boot_coverage[i] <- (mean >= boot_ci[1]) & (mean <= boot_ci[2])
}
# Compute coverage probabilities
z_prob <- mean(z_coverage)
boot_prob <- mean(boot_coverage)
# Print results
cat(paste("Large sample z CI coverage probability:", z_prob, "\n"))
cat(paste("Parametric bootstrap CI coverage probability:", boot_prob, "\n"))
set.seed(123)
library(boot)
# Function to generate data from normal distribution
generate_data <- function(n) {
rexp(n,.01)
}
# Function to compute large sample z confidence interval
z_conf_interval <- function(data, alpha) {
n <- length(data)
se <- sd(data)/sqrt(n)
z <- qnorm(1-alpha/2)
lower <- mean(data) - z*se
upper <- mean(data) + z*se
return(c(lower, upper))
}
# Function to compute parametric bootstrap confidence interval using boot function
boot_conf_interval <- function(data, alpha) {
boot_fn <- function(data, indices) {
mean(data[indices])
}
boot_samples <- boot(data, boot_fn, R=1000)
lower <- quantile(boot_samples$t, alpha/2)
upper <- quantile(boot_samples$t, 1-alpha/2)
return(c(lower, upper))
}
# Simulation settings
n <- 10
mean <- 100
sd <- 2
alpha <- 0.05
simulations <- 1000
# Initialize matrices to store coverage probabilities
z_coverage <- numeric(simulations)
boot_coverage <- numeric(simulations)
# Run Monte-Carlo simulation
for(i in 1:simulations) {
# Generate data
data <- generate_data(30)
# Compute confidence intervals
z_ci <- z_conf_interval(data, alpha)
boot_ci <- boot_conf_interval(data, alpha)
# Check if true mean is within confidence intervals
z_coverage[i] <- (mean >= z_ci[1]) & (mean <= z_ci[2])
boot_coverage[i] <- (mean >= boot_ci[1]) & (mean <= boot_ci[2])
}
# Compute coverage probabilities
z_prob <- mean(z_coverage)
boot_prob <- mean(boot_coverage)
# Print results
cat(paste("Large sample z CI coverage probability:", z_prob, "\n"))
cat(paste("Parametric bootstrap CI coverage probability:", boot_prob, "\n"))
